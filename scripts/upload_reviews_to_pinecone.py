#!/usr/bin/env python3
"""
ê°•ì˜í‰ ë°ì´í„°ë¥¼ Pineconeì— ì—…ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ì—ë¸Œë¦¬íƒ€ì„ API ì‘ë‹µ í˜•ì‹ì˜ ê°•ì˜í‰ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥
"""

import os
import sys
import json
import subprocess
from datetime import datetime
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from pinecone import Pinecone
from sentence_transformers import SentenceTransformer
import hashlib

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class VectorStore:
    """Pinecone ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        # í™˜ê²½ë³€ìˆ˜ ì²´í¬
        api_key = os.environ.get("PINECONE_API_KEY")
        if not api_key:
            raise ValueError("PINECONE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ ìƒì„±í•˜ê³  API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        
        self.pc = Pinecone(api_key=api_key)
        self.index_name = os.environ.get("PINECONE_INDEX", "courses-dev")
        self.index = self.pc.Index(self.index_name)
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        model_name = os.environ.get("EMBEDDING_MODEL", "intfloat/multilingual-e5-base")
        print(f"ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
        self.embedder = SentenceTransformer(model_name)
        print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
        print(f"âœ… VectorStore ì´ˆê¸°í™” ì™„ë£Œ - ì¸ë±ìŠ¤: {self.index_name}, ëª¨ë¸: {model_name}")

    def embed_texts(self, texts: List[str], is_query: bool = False) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            texts: ë²¡í„°í™”í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            is_query: Trueì´ë©´ ì¿¼ë¦¬ìš© (E5 ëª¨ë¸ì˜ ê²½ìš° "query:" í”„ë¦¬í”½ìŠ¤ ì¶”ê°€)
        """
        # E5 ëª¨ë¸ì¸ ê²½ìš° ì¿¼ë¦¬/íŒ¨ì‹œì§€ í”„ë¦¬í”½ìŠ¤ ì¶”ê°€
        model_name = os.environ.get("EMBEDDING_MODEL", "intfloat/multilingual-e5-base")
        if "e5" in model_name.lower() or "multilingual-e5" in model_name.lower():
            if is_query:
                # ì¿¼ë¦¬ìš©: "query:" í”„ë¦¬í”½ìŠ¤ ì¶”ê°€
                prefixed_texts = [f"query: {text}" for text in texts]
            else:
                # íŒ¨ì‹œì§€ìš©: "passage:" í”„ë¦¬í”½ìŠ¤ ì¶”ê°€ (ì €ì¥ ì‹œì™€ ë™ì¼í•˜ê²Œ)
                prefixed_texts = [f"passage: {text}" for text in texts]
        else:
            prefixed_texts = texts
        
        embeddings = self.embedder.encode(prefixed_texts, normalize_embeddings=True)
        return embeddings.tolist()

    def sanitize_id(self, text: str) -> str:
        """í•œê¸€ ë“± ë¹„ASCII ë¬¸ìë¥¼ ì•ˆì „í•œ ASCII IDë¡œ ë³€í™˜"""
        return hashlib.md5(text.encode("utf-8")).hexdigest()

    def upsert_reviews(self, review_items: List[Dict[str, Any]], namespace: Optional[str] = None) -> bool:
        """ê°•ì˜í‰ ë°ì´í„°ë¥¼ Pineconeì— ì €ì¥"""
        try:
            if not review_items:
                print("âš ï¸  ì €ì¥í•  ê°•ì˜í‰ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„ë² ë”©
            texts = [item["text"] for item in review_items]
            print(f"ğŸ“ ë²¡í„°í™”í•  í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {texts[0][:50]}..." if texts else "âŒ í…ìŠ¤íŠ¸ ì—†ìŒ")
            vectors = self.embed_texts(texts)  # is_query=False (ê¸°ë³¸ê°’)ì´ë¯€ë¡œ passage: í”„ë¦¬í”½ìŠ¤ ìë™ ì¶”ê°€
            
            # Pinecone ì—…ì„œíŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            upsert_vectors = []
            for item, vector in zip(review_items, vectors):
                # ì´ë¯¸ ASCII-safe IDë¡œ ìƒì„±ë˜ì—ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                upsert_vectors.append({
                    "id": item["id"],
                    "values": vector,
                    "metadata": item["metadata"]
                })
            
            # Pineconeì— ì—…ì„œíŠ¸
            if namespace:
                self.index.upsert(vectors=upsert_vectors, namespace=namespace)
                print(f"âœ… {len(upsert_vectors)}ê°œ ê°•ì˜í‰ì„ Pineconeì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (namespace: {namespace})")
            else:
                self.index.upsert(vectors=upsert_vectors)
                print(f"âœ… {len(upsert_vectors)}ê°œ ê°•ì˜í‰ì„ Pineconeì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (namespace: _default_)")
            return True
            
        except Exception as e:
            print(f"âŒ Pinecone ì—…ì„œíŠ¸ ì‹¤íŒ¨: {e}")
            return False

    def query_similar_reviews(self, query_text: str, top_k: int = 10, 
                            filter_dict: Optional[Dict[str, Any]] = None,
                            namespace: Optional[str] = None) -> List[Dict]:
        """ìœ ì‚¬í•œ ê°•ì˜í‰ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì„ë² ë”© (is_query=Trueë¡œ ì„¤ì •í•˜ì—¬ "query:" í”„ë¦¬í”½ìŠ¤ ìë™ ì¶”ê°€)
            query_vector = self.embed_texts([query_text], is_query=True)[0]
            
            # Pinecone ê²€ìƒ‰ ì˜µì…˜ êµ¬ì„±
            query_options = {
                "vector": query_vector,
                "top_k": top_k,
                "include_metadata": True
            }
            
            # Namespaceê°€ ì§€ì •ëœ ê²½ìš° ì¶”ê°€
            if namespace:
                query_options["namespace"] = namespace
            
            # í•„í„°ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            if filter_dict:
                query_options["filter"] = filter_dict
            
            # Pinecone ê²€ìƒ‰
            results = self.index.query(**query_options)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            similar_reviews = []
            for match in results.matches:
                similar_reviews.append({
                    "id": match.id,
                    "score": match.score,
                    "metadata": match.metadata
                })
            
            return similar_reviews
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def get_index_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ ì¡°íšŒ"""
        try:
            stats = self.index.describe_index_stats()
            return {
                "total_vector_count": stats.total_vector_count,
                "dimension": stats.dimension,
                "index_fullness": stats.index_fullness
            }
        except Exception as e:
            print(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

def korean_to_ascii(text: str) -> str:
    """í•œê¸€ì„ ASCII-safe ë¬¸ìì—´ë¡œ ë³€í™˜"""
    # ê°„ë‹¨í•œ ë§¤í•‘ í…Œì´ë¸”
    korean_map = {
        'ê¸°ê³„í•™ìŠµ': 'machine_learning',
        'ì†ê²½ì•„': 'son_kyung_ah',
        'ì†Œí”„íŠ¸ì›¨ì–´í•™ê³¼': 'software_engineering',
        'ìœ ì¢…ë¹ˆ': 'yoo_jongbin',
        'ì¸ê³µì§€ëŠ¥': 'artificial_intelligence'
    }
    
    result = text
    for korean, ascii_text in korean_map.items():
        result = result.replace(korean, ascii_text)
    
    # ë‚˜ë¨¸ì§€ í•œê¸€ì€ í•´ì‹œë¡œ ë³€í™˜
    if any('\uac00' <= char <= '\ud7af' for char in result):
        result = hashlib.md5(result.encode('utf-8')).hexdigest()[:8]
    
    return result

def execute_curl_command(curl_command: str) -> dict:
    """
    cURL ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ê³  JSON ì‘ë‹µì„ ë°˜í™˜
    
    Args:
        curl_command: ì‹¤í–‰í•  cURL ëª…ë ¹ì–´ ë¬¸ìì—´
    
    Returns:
        dict: JSON ì‘ë‹µ ë°ì´í„°
    """
    try:
        print(f"ğŸŒ cURL ëª…ë ¹ì–´ ì‹¤í–‰ ì¤‘...")
        print(f"   ëª…ë ¹ì–´: {curl_command[:100]}...")
        
        # cURL ëª…ë ¹ì–´ë¥¼ ì‰˜ì—ì„œ ì‹¤í–‰
        result = subprocess.run(
            curl_command,
            shell=True,
            capture_output=True,
            text=True,
            check=True
        )
        
        # JSON ì‘ë‹µ íŒŒì‹±
        response_data = json.loads(result.stdout)
        
        # ì‘ë‹µ êµ¬ì¡° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        print(f"âœ… cURL ì‘ë‹µ ë°›ê¸° ì™„ë£Œ")
        print(f"   ì‘ë‹µ ìµœìƒìœ„ í‚¤: {list(response_data.keys())}")
        if 'result' in response_data:
            result_keys = list(response_data['result'].keys()) if isinstance(response_data['result'], dict) else 'not a dict'
            print(f"   result í‚¤: {result_keys}")
            if isinstance(response_data['result'], dict) and 'articles' in response_data['result']:
                articles_count = len(response_data['result']['articles']) if isinstance(response_data['result']['articles'], list) else 0
                print(f"   articles ê°œìˆ˜: {articles_count}")
                if articles_count > 0:
                    first_article = response_data['result']['articles'][0]
                    print(f"   ì²« ë²ˆì§¸ article í‚¤: {list(first_article.keys()) if isinstance(first_article, dict) else 'not a dict'}")
        
        return response_data
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ cURL ëª…ë ¹ì–´ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print(f"   ì—ëŸ¬ ì¶œë ¥: {e.stderr}")
        raise
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        if 'result' in locals():
            print(f"   ì‘ë‹µ ë‚´ìš©: {result.stdout[:200]}...")
        raise

def get_existing_ids(vector_store: VectorStore, course_info: dict, namespace: Optional[str] = None) -> set:
    """
    Pineconeì—ì„œ ê¸°ì¡´ì— ì €ì¥ëœ ID ëª©ë¡ì„ ê°€ì ¸ì˜´ (ì¤‘ë³µ ê²€ì¦ìš©)
    
    Args:
        vector_store: VectorStore ì¸ìŠ¤í„´ìŠ¤
        course_info: ê°•ì˜ ì •ë³´
        namespace: ê²€ìƒ‰í•  namespace (Noneì´ë©´ _default_)
    
    Returns:
        set: ê¸°ì¡´ ID ì§‘í•©
    """
    try:
        # ê°•ì˜ëª…ê³¼ êµìˆ˜ëª…ìœ¼ë¡œ í•„í„°ë§í•˜ì—¬ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ
        course_name_ascii = korean_to_ascii(course_info['course_name'])
        professor_ascii = korean_to_ascii(course_info['professor'])
        prefix = f"{course_name_ascii}_{professor_ascii}_"
        
        # Pineconeì—ì„œ í•´ë‹¹ ê°•ì˜ì˜ ëª¨ë“  ë²¡í„° ì¡°íšŒ (í•„í„° ì‚¬ìš©)
        # ì°¸ê³ : Pineconeì€ IDë¡œ ì§ì ‘ ì¡°íšŒí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ë©”íƒ€ë°ì´í„° í•„í„°ë¡œ ì¡°íšŒ
        existing_ids = set()
        
        # ë©”íƒ€ë°ì´í„° í•„í„°ë¡œ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì‹œë„
        try:
            results = vector_store.index.query(
                vector=[0.0] * 768,  # ë”ë¯¸ ë²¡í„° (í•„í„°ë§Œ ì‚¬ìš©)
                top_k=10000,  # ìµœëŒ€ ê°œìˆ˜
                include_metadata=True,
                filter={
                    "course_name": {"$eq": course_info['course_name']},
                    "professor": {"$eq": course_info['professor']}
                },
                namespace=namespace
            )
            
            for match in results.matches:
                existing_ids.add(match.id)
        except Exception as e:
            print(f"âš ï¸  ê¸°ì¡´ ID ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
        
        return existing_ids
    except Exception as e:
        print(f"âš ï¸  ê¸°ì¡´ ID ì¡°íšŒ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
        return set()

def get_next_sequence_number(existing_ids: set, prefix: str) -> int:
    """
    ê¸°ì¡´ IDì—ì„œ ë‹¤ìŒ ì‹œí€€ìŠ¤ ë²ˆí˜¸ë¥¼ ì°¾ìŒ
    
    Args:
        existing_ids: ê¸°ì¡´ ID ì§‘í•©
        prefix: ID ì ‘ë‘ì‚¬ (ì˜ˆ: "machine_learning_son_kyung_ah_")
    
    Returns:
        int: ë‹¤ìŒ ì‹œí€€ìŠ¤ ë²ˆí˜¸
    """
    max_num = -1
    for existing_id in existing_ids:
        if existing_id.startswith(prefix):
            try:
                # ì ‘ë‘ì‚¬ ë’¤ì˜ ìˆ«ì ì¶”ì¶œ
                suffix = existing_id[len(prefix):]
                num = int(suffix)
                max_num = max(max_num, num)
            except ValueError:
                continue
    
    return max_num + 1

def create_review_items(api_response_data: dict, course_info: dict, vector_store: VectorStore, 
                        namespace: Optional[str] = None, check_duplicates: bool = True) -> list:
    """
    API ì‘ë‹µ ë°ì´í„°ë¥¼ Pinecone ì €ì¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    articlesì—ì„œ id, year, semester, text, rateë§Œ ì¶”ì¶œ
    
    Args:
        api_response_data: ì—ë¸Œë¦¬íƒ€ì„ API ì‘ë‹µ ë°ì´í„°
        course_info: ê°•ì˜ ì •ë³´ (course_name, professor í•„ìˆ˜)
        vector_store: VectorStore ì¸ìŠ¤í„´ìŠ¤
        namespace: ê²€ìƒ‰í•  namespace (ì¤‘ë³µ ê²€ì¦ìš©)
        check_duplicates: ì¤‘ë³µ ê²€ì¦ ì—¬ë¶€
    
    Returns:
        list: Pinecone ì €ì¥ìš© ë¦¬ë·° ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
    """
    review_items = []
    
    # ë‹¤ì–‘í•œ ì‘ë‹µ í˜•íƒœ ì§€ì›
    # í˜•íƒœ 1: {"result": {"articles": [...]}}
    # í˜•íƒœ 2: {"articles": [...]}
    # í˜•íƒœ 3: {"data": {"articles": [...]}}
    articles = []
    if "result" in api_response_data and isinstance(api_response_data["result"], dict):
        articles = api_response_data["result"].get("articles", [])
    elif "articles" in api_response_data:
        articles = api_response_data["articles"]
    elif "data" in api_response_data and isinstance(api_response_data["data"], dict):
        articles = api_response_data["data"].get("articles", [])
    
    if not articles:
        print("âš ï¸  articlesë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   ì‘ë‹µ ìµœìƒìœ„ í‚¤: {list(api_response_data.keys())}")
        return []
    
    print(f"ğŸ“ {len(articles)}ê°œì˜ articles ì²˜ë¦¬ ì¤‘...")
    
    # ì¤‘ë³µ ê²€ì¦ì„ ìœ„í•œ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ
    existing_ids = set()
    existing_original_ids = set()
    existing_texts = set()
    
    if check_duplicates:
        print("ğŸ” ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì¤‘ (ì¤‘ë³µ ê²€ì¦)...")
        existing_ids = get_existing_ids(vector_store, course_info, namespace)
        print(f"   ê¸°ì¡´ ID ê°œìˆ˜: {len(existing_ids)}")
        
        # ê¸°ì¡´ ë°ì´í„°ì˜ original_idì™€ textë„ ì¡°íšŒ (ì¤‘ë³µ ê²€ì¦ìš©)
        try:
            results = vector_store.index.query(
                vector=[0.0] * 768,
                top_k=10000,
                include_metadata=True,
                filter={
                    "course_name": {"$eq": course_info['course_name']},
                    "professor": {"$eq": course_info['professor']}
                },
                namespace=namespace
            )
            for match in results.matches:
                metadata = match.metadata
                if 'original_id' in metadata:
                    existing_original_ids.add(str(metadata['original_id']))
                if 'text' in metadata:
                    existing_texts.add(metadata['text'].strip())
        except Exception as e:
            print(f"âš ï¸  ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
    
    # ID ì ‘ë‘ì‚¬ ìƒì„± (ê¸°ì¡´ íŒ¨í„´ê³¼ ì¼ì¹˜)
    course_name_ascii = korean_to_ascii(course_info['course_name'])
    professor_ascii = korean_to_ascii(course_info['professor'])
    id_prefix = f"{course_name_ascii}_{professor_ascii}_"
    
    # ë‹¤ìŒ ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì°¾ê¸°
    next_seq = get_next_sequence_number(existing_ids, id_prefix)
    current_seq = next_seq
    
    duplicate_count = 0
    
    for idx, article in enumerate(articles):
        # í•„ìš”í•œ í•„ë“œë§Œ ì¶”ì¶œ: id, year, semester, text, rate
        article_id = article.get("id")
        year = article.get("year")
        semester = article.get("semester")
        text = article.get("text", "").strip()
        rate = article.get("rate")
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if None in [article_id, year, semester, rate]:
            print(f"âš ï¸  {idx}ë²ˆì§¸ articleì—ì„œ í•„ìˆ˜ í•„ë“œ ëˆ„ë½, ê±´ë„ˆëœ€")
            continue
        
        # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê±´ë„ˆëœ€ (ë²¡í„°í™” ë¶ˆê°€)
        if not text:
            print(f"âš ï¸  {idx}ë²ˆì§¸ articleì—ì„œ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ, ê±´ë„ˆëœ€")
            continue
        
        # ì¤‘ë³µ ê²€ì¦
        if check_duplicates:
            # original_idë¡œ ì¤‘ë³µ í™•ì¸
            if str(article_id) in existing_original_ids:
                print(f"âš ï¸  {idx}ë²ˆì§¸ article ì¤‘ë³µ (original_id: {article_id}), ê±´ë„ˆëœ€")
                duplicate_count += 1
                continue
            
            # í…ìŠ¤íŠ¸ë¡œ ì¤‘ë³µ í™•ì¸
            if text in existing_texts:
                print(f"âš ï¸  {idx}ë²ˆì§¸ article ì¤‘ë³µ (ë™ì¼í•œ í…ìŠ¤íŠ¸), ê±´ë„ˆëœ€")
                duplicate_count += 1
                continue
        
        # ë²¡í„° ID ìƒì„± (ê¸°ì¡´ íŒ¨í„´ê³¼ ì¼ì¹˜: machine_learning_son_kyung_ah_001 í˜•ì‹)
        review_id = f"{id_prefix}{current_seq:03d}"
        current_seq += 1
        
        # í•™ê¸° ì •ë³´ ì •ê·œí™”
        semester_normalized = f"{year}-{semester}"
        if semester == "ì—¬ë¦„" or semester == "summer":
            semester_normalized = f"{year}-summer"
        elif semester == "ê²¨ìš¸" or semester == "winter":
            semester_normalized = f"{year}-winter"
        elif semester == "1" or semester == 1:
            semester_normalized = f"{year}-1"
        elif semester == "2" or semester == 2:
            semester_normalized = f"{year}-2"
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì„± (í•„ìˆ˜ í•„ë“œë§Œ í¬í•¨)
        metadata = {
            "course_name": course_info["course_name"],
            "professor": course_info["professor"],
            "semester": semester_normalized,
            "year": year,
            "rating": rate,
            "original_id": article_id,
            "source": "evertime",
            "uploaded_at": datetime.now().isoformat(),
            "text": text  # ë¦¬ë·° í…ìŠ¤íŠ¸ë„ ë©”íƒ€ë°ì´í„°ì— í¬í•¨
        }
        
        # ì„ íƒì  í•„ë“œ ì¶”ê°€ (ìˆëŠ” ê²½ìš°ë§Œ)
        if "department" in course_info:
            metadata["department"] = course_info["department"]
        
        # ë¦¬ë·° ì•„ì´í…œ ìƒì„±
        review_item = {
            "id": review_id,
            "text": text,
            "metadata": metadata
        }
        
        review_items.append(review_item)
    
    if check_duplicates and duplicate_count > 0:
        print(f"âš ï¸  ì´ {duplicate_count}ê°œì˜ ì¤‘ë³µ ê°•ì˜í‰ ê±´ë„ˆëœ€")
    
    return review_items

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê°•ì˜í‰ ë°ì´í„° Pinecone ì—…ë¡œë“œ ì‹œì‘")
    print("=" * 60)
    
    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    
    # ==========================================
    # í•˜ë“œì½”ë”© ì„¹ì…˜ - ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”
    # ==========================================
    
    # ê°•ì˜ ì •ë³´ (í•„ìˆ˜: course_name, professor)
    course_info = {
        "course_name": "ê°•ì˜ëª…",  # ê°•ì˜ëª… (í•„ìˆ˜)
        "professor": "êµìˆ˜ëª…",      # êµìˆ˜ëª… (í•„ìˆ˜)
        # "department": "ì†Œí”„íŠ¸ì›¨ì–´í•™ê³¼",  # í•™ê³¼ (ì„ íƒì‚¬í•­)
    }
    
    # cURL ëª…ë ¹ì–´ (ì—¬ê¸°ì— ì‹¤ì œ cURL ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”)
    # ì—¬ëŸ¬ ì¤„ë¡œ ì‘ì„± ê°€ëŠ¥ (ë°±ìŠ¬ë˜ì‹œë¡œ ì¤„ë°”ê¿ˆ)
    curl_command = """curl ëª…ë ¹ì–´"""
    
    # ==========================================
    
    
    try:
        # cURL ëª…ë ¹ì–´ë¡œ API í˜¸ì¶œ
        print("ğŸŒ API í˜¸ì¶œ ì¤‘...")
        api_response_data = execute_curl_command(curl_command)
        
        # VectorStore ì´ˆê¸°í™”
        print("ğŸ”§ VectorStore ì´ˆê¸°í™” ì¤‘...")
        vector_store = VectorStore()
        
        # ê°•ì˜ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“š ê°•ì˜ ì •ë³´:")
        print(f"   - ê°•ì˜ëª…: {course_info['course_name']}")
        print(f"   - êµìˆ˜ëª…: {course_info['professor']}")
        if "department" in course_info:
            print(f"   - í•™ê³¼: {course_info['department']}")
        
        # Namespace ì„¤ì • (ê¸°ë³¸ê°’: _default_)
        namespace = os.getenv('PINE_NS') or None
        
        # API ì‘ë‹µ ë°ì´í„° ë³€í™˜
        print("\nğŸ”„ ê°•ì˜í‰ ë°ì´í„° ë³€í™˜ ì¤‘...")
        review_items = create_review_items(
            api_response_data, 
            course_info, 
            vector_store,
            namespace=namespace,
            check_duplicates=True  # ì¤‘ë³µ ê²€ì¦ í™œì„±í™”
        )
        print(f"âœ… {len(review_items)}ê°œ ê°•ì˜í‰ ë°ì´í„° ë³€í™˜ ì™„ë£Œ (ìƒˆë¡œ ì¶”ê°€ë  ë°ì´í„°)")
        
        # Pineconeì— ì—…ë¡œë“œ
        print("ğŸ“¤ Pineconeì— ì—…ë¡œë“œ ì¤‘...")
        success = vector_store.upsert_reviews(review_items, namespace=namespace)
        
        if success:
            print("=" * 60)
            print("ğŸ‰ ê°•ì˜í‰ ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ!")
            
            # ì¸ë±ìŠ¤ í†µê³„ ì¶œë ¥
            stats = vector_store.get_index_stats()
            if stats:
                print(f"ğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
                print(f"   - ì´ ë²¡í„° ìˆ˜: {stats.get('total_vector_count', 'N/A')}")
                print(f"   - ë²¡í„° ì°¨ì›: {stats.get('dimension', 'N/A')}")
                print(f"   - ì¸ë±ìŠ¤ ì‚¬ìš©ë¥ : {stats.get('index_fullness', 'N/A')}")
            
            # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
            test_query = "íŒ€í”„ë¡œì íŠ¸ê°€ ìˆëŠ” ê°•ì˜"
            similar_reviews = vector_store.query_similar_reviews(
                test_query, 
                top_k=3,
                filter_dict={"course_name": course_info["course_name"]}
            )
            
            print(f"   ì¿¼ë¦¬: '{test_query}'")
            print(f"   ê²°ê³¼: {len(similar_reviews)}ê°œ ìœ ì‚¬ ê°•ì˜í‰ ë°œê²¬")
            for i, review in enumerate(similar_reviews, 1):
                # ë©”íƒ€ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì •ë³´ ì¶œë ¥
                metadata = review['metadata']
                course_name = metadata.get('course_name', 'Unknown')
                professor = metadata.get('professor', 'Unknown')
                rating = metadata.get('rating', 0)
                text = metadata.get('text', '')[:50] + '...' if metadata.get('text') else 'No text'
                print(f"   {i}. ì ìˆ˜: {review['score']:.3f} - {course_name}({professor}) í‰ì :{rating}")
                print(f"      ë‚´ìš©: {text}")
                
        else:
            print("âŒ ì—…ë¡œë“œ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
