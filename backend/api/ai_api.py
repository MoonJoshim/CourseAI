#!/usr/bin/env python3
"""
ì—ë¸Œë¦¬íƒ€ì„ AI ì±—ë´‡ API ì„œë²„
OpenAI GPT-4 ë˜ëŠ” Google Geminië¥¼ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ë° Function Calling
"""

from flask import Flask, jsonify, request
from flask_cors import CORS
import os
import json
import sys
from dotenv import load_dotenv
from datetime import datetime
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from backend.api import get_mongo_db
from backend.api.lecture_api import search_lecture, get_or_create_driver, ensure_logged_in

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
CORS(app)

# LLM Provider ì„¤ì •
LLM_PROVIDER = os.getenv('LLM_PROVIDER', 'gemini').lower()  # ê¸°ë³¸ê°’: gemini

# LLM ì„¤ì •
if LLM_PROVIDER == 'openai':
    import openai
    openai.api_key = os.getenv('OPENAI_API_KEY')
    openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
elif LLM_PROVIDER == 'gemini':
    from google import genai
    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    gemini_client = genai.Client(api_key=GEMINI_API_KEY)
else:
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM Provider: {LLM_PROVIDER}. 'openai' ë˜ëŠ” 'gemini'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")

# ì±—ë´‡ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì—ë¸Œë¦¬íƒ€ì„ ê°•ì˜í‰ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ğŸ¯ **ì—­í• :**
- ëŒ€í•™ìƒë“¤ì˜ ìˆ˜ê°•ì‹ ì²­ì„ ë„ì™€ì£¼ëŠ” ì¹œê·¼í•œ AI ë¹„ì„œ
- ê°•ì˜í‰ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì ì´ê³  ìœ ìš©í•œ ì •ë³´ ì œê³µ
- ê°œì¸ì˜ í•™ìŠµ ìŠ¤íƒ€ì¼ê³¼ ëª©í‘œë¥¼ ê³ ë ¤í•œ ë§ì¶¤ ì¶”ì²œ

ğŸ’¬ **ëŒ€í™” ìŠ¤íƒ€ì¼:**
- ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ëŒ€í™”
- ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ì¹œê·¼ê° í‘œí˜„
- ë³µì¡í•œ ì •ë³´ë¥¼ ì´í•´í•˜ê¸° ì‰½ê²Œ ì •ë¦¬
- ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ë¼ê³  ê²©ë ¤

ğŸ”§ **ê¸°ëŠ¥:**
- ê°•ì˜ ê²€ìƒ‰ ë° ìƒì„¸ ì •ë³´ ì œê³µ
- ê°•ì˜ ë¹„êµ ë° ì¶”ì²œ
- êµìˆ˜ë‹˜ë³„ ê°•ì˜ ìŠ¤íƒ€ì¼ ë¶„ì„
- ìˆ˜ê°• íŒ ë° ì¡°ì–¸ ì œê³µ

ì‚¬ìš©ìê°€ ê°•ì˜ì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´, ì ì ˆí•œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ í›„ ì¹œê·¼í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
"""

# Function Calling ì •ì˜ (OpenAI í˜•ì‹)
CHATBOT_FUNCTIONS_OPENAI = [
    {
        "name": "search_lecture",
        "description": "ê°•ì˜ëª…ì´ë‚˜ êµìˆ˜ëª…ìœ¼ë¡œ ì—ë¸Œë¦¬íƒ€ì„ì—ì„œ ê°•ì˜ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤",
        "parameters": {
            "type": "object",
            "properties": {
                "keyword": {
                    "type": "string",
                    "description": "ê²€ìƒ‰í•  ê°•ì˜ëª… ë˜ëŠ” êµìˆ˜ëª…"
                }
            },
            "required": ["keyword"]
        }
    },
    {
        "name": "compare_lectures",
        "description": "ì—¬ëŸ¬ ê°•ì˜ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤",
        "parameters": {
            "type": "object",
            "properties": {
                "keywords": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ë¹„êµí•  ê°•ì˜ëª…ë“¤ì˜ ë°°ì—´"
                }
            },
            "required": ["keywords"]
        }
    },
    {
        "name": "get_recommendations",
        "description": "íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” ê°•ì˜ ì¶”ì²œì„ ìœ„í•´ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤",
        "parameters": {
            "type": "object",
            "properties": {
                "category": {
                    "type": "string",
                    "description": "ì¶”ì²œë°›ê³  ì‹¶ì€ ë¶„ì•¼ë‚˜ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ì „ê³µ, êµì–‘, í”„ë¡œê·¸ë˜ë° ë“±)"
                },
                "keywords": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ê²€ìƒ‰í•  ê´€ë ¨ í‚¤ì›Œë“œë“¤"
                }
            },
            "required": ["category", "keywords"]
        }
    }
]

# Function Calling ì •ì˜ (Gemini í˜•ì‹)
CHATBOT_TOOLS_GEMINI = [
    {
        "function_declarations": [
            {
                "name": "search_lecture",
                "description": "ê°•ì˜ëª…ì´ë‚˜ êµìˆ˜ëª…ìœ¼ë¡œ ì—ë¸Œë¦¬íƒ€ì„ì—ì„œ ê°•ì˜ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "keyword": {
                            "type": "string",
                            "description": "ê²€ìƒ‰í•  ê°•ì˜ëª… ë˜ëŠ” êµìˆ˜ëª…"
                        }
                    },
                    "required": ["keyword"]
                }
            },
            {
                "name": "compare_lectures",
                "description": "ì—¬ëŸ¬ ê°•ì˜ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "keywords": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "ë¹„êµí•  ê°•ì˜ëª…ë“¤ì˜ ë°°ì—´"
                        }
                    },
                    "required": ["keywords"]
                }
            },
            {
                "name": "get_recommendations",
                "description": "íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” ê°•ì˜ ì¶”ì²œì„ ìœ„í•´ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "category": {
                            "type": "string",
                            "description": "ì¶”ì²œë°›ê³  ì‹¶ì€ ë¶„ì•¼ë‚˜ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ì „ê³µ, êµì–‘, í”„ë¡œê·¸ë˜ë° ë“±)"
                        },
                        "keywords": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "ê²€ìƒ‰í•  ê´€ë ¨ í‚¤ì›Œë“œë“¤"
                        }
                    },
                    "required": ["category", "keywords"]
                }
            }
        ]
    }
]

def handle_function_call(function_name, arguments):
    """Function Call ì²˜ë¦¬"""
    try:
        print(f"ğŸ”§ Function Call: {function_name} with args: {arguments}")
        
        if function_name == "search_lecture":
            keyword = arguments.get("keyword")
            # DBì—ì„œ ì§ì ‘ ê²€ìƒ‰ (í¬ë¡¤ë§ ë¶ˆí•„ìš”)
            from backend.api.lecture_api import search_courses_from_db
            results = search_courses_from_db(keyword)
            return {
                "success": True,
                "function": "search_lecture",
                "keyword": keyword,
                "results": results,
                "count": len(results)
            }
                
        elif function_name == "compare_lectures":
            keywords = arguments.get("keywords", [])
            all_results = {}
            
            # DBì—ì„œ ì§ì ‘ ê²€ìƒ‰ (í¬ë¡¤ë§ ë¶ˆí•„ìš”)
            from backend.api.lecture_api import search_courses_from_db
            for keyword in keywords:
                results = search_courses_from_db(keyword)
                all_results[keyword] = results
            
            return {
                "success": True,
                "function": "compare_lectures",
                "keywords": keywords,
                "results": all_results
            }
                
        elif function_name == "get_recommendations":
            category = arguments.get("category")
            keywords = arguments.get("keywords", [])
            all_results = {}
            
            # DBì—ì„œ ì§ì ‘ ê²€ìƒ‰ (í¬ë¡¤ë§ ë¶ˆí•„ìš”)
            from backend.api.lecture_api import search_courses_from_db
            for keyword in keywords:
                results = search_courses_from_db(keyword)
                all_results[keyword] = results
            
            return {
                "success": True,
                "function": "get_recommendations",
                "category": category,
                "keywords": keywords,
                "results": all_results
            }
        
        else:
            return {
                "success": False,
                "error": f"Unknown function: {function_name}"
            }
            
    except Exception as e:
        print(f"âŒ Function call error: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }

def chat_with_openai(user_message, conversation_history):
    """OpenAIë¥¼ ì‚¬ìš©í•œ ì±„íŒ…"""
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 5ê°œë§Œ)
    for hist in conversation_history[-5:]:
        messages.append({"role": "user", "content": hist.get("user", "")})
        messages.append({"role": "assistant", "content": hist.get("assistant", "")})
    
    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    messages.append({"role": "user", "content": user_message})
    
    # OpenAI API í˜¸ì¶œ
    response = openai_client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        functions=CHATBOT_FUNCTIONS_OPENAI,
        function_call="auto",
        temperature=0.7,
        max_tokens=1000
    )
    
    message = response.choices[0].message
    function_called = None
    
    # Function Call ì²˜ë¦¬
    if message.function_call:
        function_name = message.function_call.name
        function_args = json.loads(message.function_call.arguments)
        function_called = function_name
        
        print(f"ğŸ”§ Function Call ê°ì§€: {function_name}")
        
        # í•¨ìˆ˜ ì‹¤í–‰
        function_result = handle_function_call(function_name, function_args)
        
        # í•¨ìˆ˜ ê²°ê³¼ë¥¼ í¬í•¨í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±
        messages.append({
            "role": "assistant",
            "content": None,
            "function_call": {
                "name": function_name,
                "arguments": json.dumps(function_args)
            }
        })
        
        messages.append({
            "role": "function",
            "name": function_name,
            "content": json.dumps(function_result, ensure_ascii=False)
        })
        
        # ìµœì¢… ì‘ë‹µ ìƒì„±
        final_response = openai_client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            temperature=0.7,
            max_tokens=1500
        )
        
        ai_response = final_response.choices[0].message.content
        
    else:
        # ì¼ë°˜ ëŒ€í™” ì‘ë‹µ
        ai_response = message.content
    
    return ai_response, function_called

def chat_with_gemini(user_message, conversation_history):
    """Gemini(google-genai ìƒˆ SDK)ë¡œ ì±„íŒ…"""
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ contents êµ¬ì„±
    prompt_lines = [SYSTEM_PROMPT.strip(), "", "ì´ì „ ëŒ€í™”:"]
    for hist in conversation_history[-5:]:
        prompt_lines.append(f"ì‚¬ìš©ì: {hist.get('user', '')}")
        if hist.get('assistant'):
            prompt_lines.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {hist.get('assistant', '')}")
    prompt_lines.append("")
    prompt_lines.append(f"ì‚¬ìš©ì: {user_message}")
    contents = "\n".join(prompt_lines)

    # ë‹¨ì¼ í˜¸ì¶œë¡œ ì‘ë‹µ ìƒì„± (ê¸°ë³¸: ìµœì‹  ëª¨ë¸ëª… ì‚¬ìš©)
    response = gemini_client.models.generate_content(
        model="gemini-2.5-flash",
        contents=contents,
    )

    ai_response = getattr(response, 'text', None) or str(response)
    function_called = None
    return ai_response, function_called

@app.route('/api/chat', methods=['POST'])
def chat():
    """AI ì±—ë´‡ ëŒ€í™” API"""
    try:
        data = request.json
        user_message = data.get('message', '').strip()
        conversation_history = data.get('history', [])
        
        if not user_message:
            return jsonify({'error': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”'}), 400
        
        print(f"ğŸ’¬ ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}")
        print(f"ğŸ¤– LLM Provider: {LLM_PROVIDER}")
        
        # LLM Providerì— ë”°ë¼ ë‹¤ë¥¸ í•¨ìˆ˜ í˜¸ì¶œ
        if LLM_PROVIDER == 'openai':
            ai_response, function_called = chat_with_openai(user_message, conversation_history)
        elif LLM_PROVIDER == 'gemini':
            ai_response, function_called = chat_with_gemini(user_message, conversation_history)
        else:
            return jsonify({'error': f'ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM Provider: {LLM_PROVIDER}'}), 400
        
        print(f"ğŸ¤– AI ì‘ë‹µ: {ai_response[:100]}...")
        
        return jsonify({
            'response': ai_response,
            'timestamp': datetime.now().isoformat(),
            'function_called': function_called,
            'llm_provider': LLM_PROVIDER
        })
        
    except Exception as e:
        print(f"âŒ ì±—ë´‡ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'ì±—ë´‡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}), 500

@app.route('/api/chat/test', methods=['GET'])
def test_chat():
    """ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    test_messages = [
        "ë°ì´í„°ë² ì´ìŠ¤ ê°•ì˜í‰ ì•Œë ¤ì¤˜",
        "ì»´ê³µ ì¶”ì²œ ê³¼ëª©ì€?",
        "ì›¹í”„ë¡œê·¸ë˜ë°ì´ë‘ ëª¨ë°”ì¼í”„ë¡œê·¸ë˜ë° ì¤‘ì— ë­ê°€ ë‚˜ì„ê¹Œ?"
    ]
    
    return jsonify({
        'message': 'ì±—ë´‡ APIê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!',
        'test_queries': test_messages,
        'endpoints': {
            'chat': 'POST /api/chat',
            'test': 'GET /api/chat/test'
        }
    })

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    provider_name = "Gemini" if LLM_PROVIDER == "gemini" else "OpenAI GPT-4"
    return f'''
    <h1>ğŸ¤– ì—ë¸Œë¦¬íƒ€ì„ AI ì±—ë´‡ API</h1>
    <p><strong>{provider_name}</strong>ë¥¼ ì´ìš©í•œ ê°•ì˜í‰ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸</p>
    <p>í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ LLM: <strong>{LLM_PROVIDER.upper()}</strong></p>
    
    <h2>ê¸°ëŠ¥:</h2>
    <ul>
        <li>ğŸ” ìì—°ì–´ë¡œ ê°•ì˜ ê²€ìƒ‰</li>
        <li>ğŸ“Š ê°•ì˜ ë¹„êµ ë° ë¶„ì„</li>
        <li>ğŸ’¡ ê°œì¸ ë§ì¶¤ ì¶”ì²œ</li>
        <li>ğŸ’¬ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤</li>
    </ul>
    
    <h2>ì‚¬ìš©ë²•:</h2>
    <ul>
        <li><code>POST /api/chat</code> - AI ì±—ë´‡ ëŒ€í™”</li>
        <li><code>GET /api/chat/test</code> - í…ŒìŠ¤íŠ¸ í˜ì´ì§€</li>
    </ul>
    
    <h2>ì˜ˆì‹œ ì§ˆë¬¸:</h2>
    <ul>
        <li>"ë°ì´í„°ë² ì´ìŠ¤ ê°•ì˜í‰ ì•Œë ¤ì¤˜"</li>
        <li>"ì»´ê³µì—ì„œ ê¿€ê°• ì¶”ì²œí•´ì¤˜"</li>
        <li>"ê¹€êµìˆ˜ë‹˜ ê°•ì˜ ì–´ë–¤ì§€ ê¶ê¸ˆí•´"</li>
    </ul>
    
    <h2>ì„¤ì • ë³€ê²½:</h2>
    <p>í™˜ê²½ë³€ìˆ˜ <code>LLM_PROVIDER</code>ë¥¼ ì„¤ì •í•˜ì—¬ LLMì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (gemini ë˜ëŠ” openai)</p>
    '''

@app.route('/api/health/db', methods=['GET'])
def health_db():
    """MongoDB ì—°ê²° í—¬ìŠ¤ì²´í¬"""
    try:
        db = get_mongo_db()
        result = db.command('ping')
        return jsonify({'ok': True, 'result': result}), 200
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)}), 500

if __name__ == '__main__':
    provider_name = "Gemini" if LLM_PROVIDER == "gemini" else "OpenAI GPT-4"
    print("ğŸ¤– ì—ë¸Œë¦¬íƒ€ì„ AI ì±—ë´‡ API ì„œë²„ ì‹œì‘")
    print("ğŸ“ http://localhost:5003")
    print(f"ğŸ§  {provider_name} Function Calling í™œì„±í™”")
    print(f"ğŸ”§ LLM Provider: {LLM_PROVIDER.upper()}")
    
    app.run(debug=True, host='0.0.0.0', port=5003)
