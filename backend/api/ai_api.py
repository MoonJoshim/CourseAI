#!/usr/bin/env python3
"""
ì—ë¸Œë¦¬íƒ€ì„ AI ì±—ë´‡ API ì„œë²„
OpenAI GPT-4 ë˜ëŠ” Google Geminië¥¼ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ë° Function Calling
"""

from flask import Flask, jsonify, request
from flask_cors import CORS
import os
import json
import sys
from dotenv import load_dotenv
from datetime import datetime
from typing import List, Dict, Optional
import google.generativeai as genai
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from backend.api import get_mongo_db
from backend.api.lecture_api import search_lecture, get_or_create_driver, ensure_logged_in

# VectorStoreë¥¼ ì§ì ‘ íŒŒì¼ ê²½ë¡œë¡œ importí•˜ì—¬ __init__.pyì˜ database ì´ˆê¸°í™”ë¥¼ í”¼í•¨
VectorStore = None
vector_store = None
try:
    import importlib.util

    vector_store_path = os.path.join(os.path.dirname(__file__), '..', 'models', 'vector_store.py')
    spec = importlib.util.spec_from_file_location("vector_store", vector_store_path)
    vector_store_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(vector_store_module)
    VectorStore = getattr(vector_store_module, "VectorStore", None)

    if VectorStore is not None:
        try:
            vector_store = VectorStore()
            print("âœ… Pinecone VectorStore ì´ˆê¸°í™” ì™„ë£Œ (RAG ê¸°ëŠ¥ í™œì„±í™”)")
        except Exception as e:  # pylint: disable=broad-except
            print(f"âš ï¸ VectorStore ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            vector_store = None
    else:
        print("âš ï¸ VectorStore í´ë˜ìŠ¤ë¥¼ vector_store ëª¨ë“ˆì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ ë¹„í™œì„±í™”.")
except Exception as e:  # pylint: disable=broad-except
    # sentence_transformers, pinecone ë“±ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ë„ ê¸°ë³¸ ì±—ë´‡ì€ ë™ì‘í•˜ë„ë¡ RAGë§Œ ë¹„í™œì„±í™”
    print(f"âš ï¸ VectorStore ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    VectorStore = None
    vector_store = None

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
CORS(app)

# LLM Provider ì„¤ì •
LLM_PROVIDER = os.getenv('LLM_PROVIDER', 'gemini').lower()  # ê¸°ë³¸ê°’: gemini

# LLM ì„¤ì •
if LLM_PROVIDER == 'openai':
    import openai
    openai.api_key = os.getenv('OPENAI_API_KEY')
    openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
elif LLM_PROVIDER == 'gemini':
    # genaiëŠ” ì´ë¯¸ 14ë²ˆ ì¤„ì—ì„œ import ë˜ì—ˆìŒ
    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY') or os.getenv('GOOGLE_API_KEY') or os.getenv('GOOGLE_GEMINI_API_KEY')
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=GEMINI_API_KEY)
else:
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM Provider: {LLM_PROVIDER}. 'openai' ë˜ëŠ” 'gemini'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")

# ì±—ë´‡ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì—ë¸Œë¦¬íƒ€ì„ ê°•ì˜í‰ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ğŸ¯ **ì—­í• :**
- ëŒ€í•™ìƒë“¤ì˜ ìˆ˜ê°•ì‹ ì²­ì„ ë„ì™€ì£¼ëŠ” ì¹œê·¼í•œ AI ë¹„ì„œ
- ê°•ì˜í‰ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì ì´ê³  ìœ ìš©í•œ ì •ë³´ ì œê³µ
- ê°œì¸ì˜ í•™ìŠµ ìŠ¤íƒ€ì¼ê³¼ ëª©í‘œë¥¼ ê³ ë ¤í•œ ë§ì¶¤ ì¶”ì²œ

ğŸ’¬ **ëŒ€í™” ìŠ¤íƒ€ì¼:**
- ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ëŒ€í™”
- ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ì¹œê·¼ê° í‘œí˜„
- ë³µì¡í•œ ì •ë³´ë¥¼ ì´í•´í•˜ê¸° ì‰½ê²Œ ì •ë¦¬
- ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ë¼ê³  ê²©ë ¤

ğŸ”§ **ê¸°ëŠ¥:**
- ê°•ì˜ ê²€ìƒ‰ ë° ìƒì„¸ ì •ë³´ ì œê³µ
- ê°•ì˜ ë¹„êµ ë° ì¶”ì²œ
- êµìˆ˜ë‹˜ë³„ ê°•ì˜ ìŠ¤íƒ€ì¼ ë¶„ì„
- ìˆ˜ê°• íŒ ë° ì¡°ì–¸ ì œê³µ

ì‚¬ìš©ìê°€ ê°•ì˜ì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´, ì ì ˆí•œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ í›„ ì¹œê·¼í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
"""

# RAG ì±—ë´‡ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
RAG_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì—ë¸Œë¦¬íƒ€ì„ ê°•ì˜í‰ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ëœ ê°•ì˜í‰ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ğŸ¯ **ì—­í• :**
- ëŒ€í•™ìƒë“¤ì˜ ìˆ˜ê°•ì‹ ì²­ì„ ë„ì™€ì£¼ëŠ” ì¹œê·¼í•œ AI ë¹„ì„œ
- ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì€ ê°•ì˜í‰ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì ì´ê³  ìœ ìš©í•œ ì •ë³´ ì œê³µ
- ê°œì¸ì˜ í•™ìŠµ ìŠ¤íƒ€ì¼ê³¼ ëª©í‘œë¥¼ ê³ ë ¤í•œ ë§ì¶¤ ì¶”ì²œ

ğŸ’¬ **ëŒ€í™” ìŠ¤íƒ€ì¼:**
- ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ëŒ€í™”
- ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ì¹œê·¼ê° í‘œí˜„
- ë³µì¡í•œ ì •ë³´ë¥¼ ì´í•´í•˜ê¸° ì‰½ê²Œ ì •ë¦¬
- ê²€ìƒ‰ëœ ê°•ì˜í‰ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ ë‹µë³€

ğŸ”§ **ì¤‘ìš” ì§€ì¹¨:**
- ì œê³µëœ ê°•ì˜í‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ê°•ì˜í‰ ë°ì´í„°ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
- ì—¬ëŸ¬ ê°•ì˜í‰ì˜ ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ê· í˜•ì¡íŒ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ ê°•ì˜ëª…, êµìˆ˜ëª…, í‰ì  ë“±ì€ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”
"""

# Function Calling ì •ì˜ (OpenAI í˜•ì‹)
CHATBOT_FUNCTIONS_OPENAI = [
    {
        "name": "search_lecture",
        "description": "ê°•ì˜ëª…ì´ë‚˜ êµìˆ˜ëª…ìœ¼ë¡œ ì—ë¸Œë¦¬íƒ€ì„ì—ì„œ ê°•ì˜ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤",
        "parameters": {
            "type": "object",
            "properties": {
                "keyword": {
                    "type": "string",
                    "description": "ê²€ìƒ‰í•  ê°•ì˜ëª… ë˜ëŠ” êµìˆ˜ëª…"
                }
            },
            "required": ["keyword"]
        }
    },
    {
        "name": "compare_lectures",
        "description": "ì—¬ëŸ¬ ê°•ì˜ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤",
        "parameters": {
            "type": "object",
            "properties": {
                "keywords": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ë¹„êµí•  ê°•ì˜ëª…ë“¤ì˜ ë°°ì—´"
                }
            },
            "required": ["keywords"]
        }
    },
    {
        "name": "get_recommendations",
        "description": "íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” ê°•ì˜ ì¶”ì²œì„ ìœ„í•´ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤",
        "parameters": {
            "type": "object",
            "properties": {
                "category": {
                    "type": "string",
                    "description": "ì¶”ì²œë°›ê³  ì‹¶ì€ ë¶„ì•¼ë‚˜ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ì „ê³µ, êµì–‘, í”„ë¡œê·¸ë˜ë° ë“±)"
                },
                "keywords": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ê²€ìƒ‰í•  ê´€ë ¨ í‚¤ì›Œë“œë“¤"
                }
            },
            "required": ["category", "keywords"]
        }
    }
]

# Function Calling ì •ì˜ (Gemini í˜•ì‹)
CHATBOT_TOOLS_GEMINI = [
    {
        "function_declarations": [
            {
                "name": "search_lecture",
                "description": "ê°•ì˜ëª…ì´ë‚˜ êµìˆ˜ëª…ìœ¼ë¡œ ì—ë¸Œë¦¬íƒ€ì„ì—ì„œ ê°•ì˜ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "keyword": {
                            "type": "string",
                            "description": "ê²€ìƒ‰í•  ê°•ì˜ëª… ë˜ëŠ” êµìˆ˜ëª…"
                        }
                    },
                    "required": ["keyword"]
                }
            },
            {
                "name": "compare_lectures",
                "description": "ì—¬ëŸ¬ ê°•ì˜ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "keywords": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "ë¹„êµí•  ê°•ì˜ëª…ë“¤ì˜ ë°°ì—´"
                        }
                    },
                    "required": ["keywords"]
                }
            },
            {
                "name": "get_recommendations",
                "description": "íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” ê°•ì˜ ì¶”ì²œì„ ìœ„í•´ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "category": {
                            "type": "string",
                            "description": "ì¶”ì²œë°›ê³  ì‹¶ì€ ë¶„ì•¼ë‚˜ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ì „ê³µ, êµì–‘, í”„ë¡œê·¸ë˜ë° ë“±)"
                        },
                        "keywords": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "ê²€ìƒ‰í•  ê´€ë ¨ í‚¤ì›Œë“œë“¤"
                        }
                    },
                    "required": ["category", "keywords"]
                }
            }
        ]
    }
]

def handle_function_call(function_name, arguments):
    """Function Call ì²˜ë¦¬"""
    try:
        print(f"ğŸ”§ Function Call: {function_name} with args: {arguments}")
        
        if function_name == "search_lecture":
            keyword = arguments.get("keyword")
            # DBì—ì„œ ì§ì ‘ ê²€ìƒ‰ (í¬ë¡¤ë§ ë¶ˆí•„ìš”)
            from backend.api.lecture_api import search_courses_from_db
            results = search_courses_from_db(keyword)
            return {
                "success": True,
                "function": "search_lecture",
                "keyword": keyword,
                "results": results,
                "count": len(results)
            }
                
        elif function_name == "compare_lectures":
            keywords = arguments.get("keywords", [])
            all_results = {}
            
            # DBì—ì„œ ì§ì ‘ ê²€ìƒ‰ (í¬ë¡¤ë§ ë¶ˆí•„ìš”)
            from backend.api.lecture_api import search_courses_from_db
            for keyword in keywords:
                results = search_courses_from_db(keyword)
                all_results[keyword] = results
            
            return {
                "success": True,
                "function": "compare_lectures",
                "keywords": keywords,
                "results": all_results
            }
                
        elif function_name == "get_recommendations":
            category = arguments.get("category")
            keywords = arguments.get("keywords", [])
            all_results = {}
            
            # DBì—ì„œ ì§ì ‘ ê²€ìƒ‰ (í¬ë¡¤ë§ ë¶ˆí•„ìš”)
            from backend.api.lecture_api import search_courses_from_db
            for keyword in keywords:
                results = search_courses_from_db(keyword)
                all_results[keyword] = results
            
            return {
                "success": True,
                "function": "get_recommendations",
                "category": category,
                "keywords": keywords,
                "results": all_results
            }
        
        else:
            return {
                "success": False,
                "error": f"Unknown function: {function_name}"
            }
            
    except Exception as e:
        print(f"âŒ Function call error: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }

def chat_with_openai(user_message, conversation_history):
    """OpenAIë¥¼ ì‚¬ìš©í•œ ì±„íŒ…"""
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 5ê°œë§Œ)
    for hist in conversation_history[-5:]:
        messages.append({"role": "user", "content": hist.get("user", "")})
        messages.append({"role": "assistant", "content": hist.get("assistant", "")})
    
    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    messages.append({"role": "user", "content": user_message})
    
    # OpenAI API í˜¸ì¶œ
    response = openai_client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        functions=CHATBOT_FUNCTIONS_OPENAI,
        function_call="auto",
        temperature=0.7,
        max_tokens=1000
    )
    
    message = response.choices[0].message
    function_called = None
    
    # Function Call ì²˜ë¦¬
    if message.function_call:
        function_name = message.function_call.name
        function_args = json.loads(message.function_call.arguments)
        function_called = function_name
        
        print(f"ğŸ”§ Function Call ê°ì§€: {function_name}")
        
        # í•¨ìˆ˜ ì‹¤í–‰
        function_result = handle_function_call(function_name, function_args)
        
        # í•¨ìˆ˜ ê²°ê³¼ë¥¼ í¬í•¨í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±
        messages.append({
            "role": "assistant",
            "content": None,
            "function_call": {
                "name": function_name,
                "arguments": json.dumps(function_args)
            }
        })
        
        messages.append({
            "role": "function",
            "name": function_name,
            "content": json.dumps(function_result, ensure_ascii=False)
        })
        
        # ìµœì¢… ì‘ë‹µ ìƒì„±
        final_response = openai_client.chat.completions.create(
            model="gpt-4-1106-preview",
            messages=messages,
            temperature=0.7,
            max_tokens=1500
        )
        
        ai_response = final_response.choices[0].message.content
        
    else:
        # ì¼ë°˜ ëŒ€í™” ì‘ë‹µ
        ai_response = message.content
    
    return ai_response, function_called

def chat_with_gemini(user_message, conversation_history):
    """Gemini(google-genai ìƒˆ SDK)ë¡œ ì±„íŒ…"""
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ contents êµ¬ì„±
    prompt_lines = [SYSTEM_PROMPT.strip(), "", "ì´ì „ ëŒ€í™”:"]
    for hist in conversation_history[-5:]:
        prompt_lines.append(f"ì‚¬ìš©ì: {hist.get('user', '')}")
        if hist.get('assistant'):
            prompt_lines.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {hist.get('assistant', '')}")
    prompt_lines.append("")
    prompt_lines.append(f"ì‚¬ìš©ì: {user_message}")
    contents = "\n".join(prompt_lines)

    # ë‹¨ì¼ í˜¸ì¶œë¡œ ì‘ë‹µ ìƒì„± (ê¸°ë³¸: ìµœì‹  ëª¨ë¸ëª… ì‚¬ìš©)
    response = gemini_client.models.generate_content(
        model="gemini-2.5-flash",
        contents=contents,
    )

    ai_response = getattr(response, 'text', None) or str(response)
    function_called = None
    return ai_response, function_called

# ========== RAG ê´€ë ¨ í•¨ìˆ˜ë“¤ ==========

def format_context_from_reviews(reviews: List[Dict]) -> str:
    """ê²€ìƒ‰ëœ ê°•ì˜í‰ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
    if not reviews:
        return "ê´€ë ¨ ê°•ì˜í‰ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    context_parts = []
    context_parts.append("=== ê²€ìƒ‰ëœ ê°•ì˜í‰ ì»¨í…ìŠ¤íŠ¸ ===\n")
    
    for idx, review in enumerate(reviews[:5], 1):  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
        metadata = review.get('metadata', {})
        score = review.get('score', 0)
        
        course_name = metadata.get('course_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
        professor = metadata.get('professor', 'ì•Œ ìˆ˜ ì—†ìŒ')
        rating = metadata.get('rating', 'N/A')
        # textì™€ review_text ë‘˜ ë‹¤ í™•ì¸ (í•„ë“œëª… ë¶ˆì¼ì¹˜ ëŒ€ì‘)
        review_text = metadata.get('text', '') or metadata.get('review_text', '')
        semester = metadata.get('semester', '')
        
        context_parts.append(f"[{idx}] ê°•ì˜: {course_name}")
        context_parts.append(f"    êµìˆ˜: {professor}")
        context_parts.append(f"    í•™ê¸°: {semester}")
        context_parts.append(f"    í‰ì : {rating}/5.0")
        context_parts.append(f"    ê°•ì˜í‰: {review_text}")
        context_parts.append(f"    ìœ ì‚¬ë„ ì ìˆ˜: {score:.3f}")
        context_parts.append("")
    
    context_parts.append("=== ì»¨í…ìŠ¤íŠ¸ ë ===\n")
    return "\n".join(context_parts)

def chat_with_rag_openai(user_message: str, conversation_history: List[Dict], top_k: int = 5, namespace: Optional[str] = None):
    """OpenAIë¥¼ ì‚¬ìš©í•œ RAG ê¸°ë°˜ ì±„íŒ…"""
    # 1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°í™”í•˜ì—¬ ìœ ì‚¬í•œ ê°•ì˜í‰ ê²€ìƒ‰
    if vector_store:
        # namespaceê°€ Noneì´ë©´ Pineconeì´ ìë™ìœ¼ë¡œ _default_ë¥¼ ì‚¬ìš©
        actual_namespace = namespace if namespace else "_default_"
        print(f"ğŸ” Pineconeì—ì„œ ìœ ì‚¬í•œ ê°•ì˜í‰ ê²€ìƒ‰ ì¤‘... (top_k={top_k}, namespace={actual_namespace})")
        similar_reviews = vector_store.query_similar_reviews(user_message, top_k=top_k, namespace=namespace)
        print(f"âœ… {len(similar_reviews)}ê°œì˜ ìœ ì‚¬í•œ ê°•ì˜í‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        context = format_context_from_reviews(similar_reviews)
    else:
        context = "âš ï¸ VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ê°•ì˜í‰ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        similar_reviews = []
    
    # 2. ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
    messages = [{"role": "system", "content": RAG_SYSTEM_PROMPT}]
    
    # 3. ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ì¶”ê°€
    enhanced_system_prompt = f"{RAG_SYSTEM_PROMPT}\n\n{context}\n\nìœ„ì˜ ê°•ì˜í‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."
    messages[0]["content"] = enhanced_system_prompt
    
    # 4. ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 5ê°œë§Œ)
    for hist in conversation_history[-5:]:
        messages.append({"role": "user", "content": hist.get("user", "")})
        messages.append({"role": "assistant", "content": hist.get("assistant", "")})
    
    # 5. í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    messages.append({"role": "user", "content": user_message})
    
    # 6. OpenAI API í˜¸ì¶œ
    response = openai_client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=messages,
        temperature=0.7,
        max_tokens=1500
    )
    
    ai_response = response.choices[0].message.content
    
    return ai_response, similar_reviews

def chat_with_rag_gemini(user_message: str, conversation_history: List[Dict], top_k: int = 5, namespace: Optional[str] = None):
    """Geminië¥¼ ì‚¬ìš©í•œ RAG ê¸°ë°˜ ì±„íŒ…"""
    # 1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°í™”í•˜ì—¬ ìœ ì‚¬í•œ ê°•ì˜í‰ ê²€ìƒ‰
    if vector_store:
        # namespaceê°€ Noneì´ë©´ Pineconeì´ ìë™ìœ¼ë¡œ _default_ë¥¼ ì‚¬ìš©
        actual_namespace = namespace if namespace else "_default_"
        print(f"ğŸ” Pineconeì—ì„œ ìœ ì‚¬í•œ ê°•ì˜í‰ ê²€ìƒ‰ ì¤‘... (top_k={top_k}, namespace={actual_namespace})")
        similar_reviews = vector_store.query_similar_reviews(user_message, top_k=top_k, namespace=namespace)
        print(f"âœ… {len(similar_reviews)}ê°œì˜ ìœ ì‚¬í•œ ê°•ì˜í‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        context = format_context_from_reviews(similar_reviews)
    else:
        context = "âš ï¸ VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ê°•ì˜í‰ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        similar_reviews = []
    
    # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_lines = [RAG_SYSTEM_PROMPT.strip(), "", context, "", "ì´ì „ ëŒ€í™”:"]
    
    for hist in conversation_history[-5:]:
        prompt_lines.append(f"ì‚¬ìš©ì: {hist.get('user', '')}")
        if hist.get('assistant'):
            prompt_lines.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {hist.get('assistant', '')}")
    
    prompt_lines.append("")
    prompt_lines.append(f"ì‚¬ìš©ì: {user_message}")
    prompt_lines.append("\nìœ„ì˜ ê°•ì˜í‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.")
    
    contents = "\n".join(prompt_lines)
    
    # 3. Gemini API í˜¸ì¶œ
    response = gemini_client.models.generate_content(
        model="gemini-2.5-flash",
        contents=contents,
    )
    
    ai_response = getattr(response, 'text', None) or str(response)
    
    return ai_response, similar_reviews

@app.route('/api/chat', methods=['POST'])
def chat():
    """AI ì±—ë´‡ ëŒ€í™” API"""
    try:
        data = request.json
        user_message = data.get('message', '').strip()
        conversation_history = data.get('history', [])
        
        if not user_message:
            return jsonify({'error': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”'}), 400
        
        print(f"ğŸ’¬ ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}")
        print(f"ğŸ¤– LLM Provider: {LLM_PROVIDER}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        
        # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 5ê°œë§Œ)
        for hist in conversation_history[-5:]:
            messages.append({"role": "user", "content": hist.get("user", "")})
            messages.append({"role": "assistant", "content": hist.get("assistant", "")})
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append({"role": "user", "content": user_message})
        
        # Gemini API í˜¸ì¶œ (ë‹¨ìˆœ ëŒ€í™”)
        model = genai.GenerativeModel("gemini-1.5-flash")
        # íˆìŠ¤í† ë¦¬ë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ì—°ê²°
        history_text = "\n".join([
            f"ì‚¬ìš©ì: {h.get('user','')}\nì–´ì‹œìŠ¤í„´íŠ¸: {h.get('assistant','')}" for h in conversation_history[-5:]
        ])
        prompt = f"""
{SYSTEM_PROMPT}

ì´ì „ ëŒ€í™”(ìˆìœ¼ë©´):
{history_text}

ìƒˆ ì‚¬ìš©ì ë©”ì‹œì§€:
{user_message}
""".strip()

        gen = model.generate_content(prompt)
        ai_response = gen.text if hasattr(gen, 'text') else str(gen)
        
        print(f"ğŸ¤– AI ì‘ë‹µ: {ai_response[:100]}...")
        
        return jsonify({
            'response': ai_response,
            'timestamp': datetime.now().isoformat(),
            'model': 'gemini-1.5-flash'
        })
        
    except Exception as e:
        print(f"âŒ ì±—ë´‡ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'ì±—ë´‡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}), 500

@app.route('/api/chat/test', methods=['GET'])
def test_chat():
    """ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    test_messages = [
        "ë°ì´í„°ë² ì´ìŠ¤ ê°•ì˜í‰ ì•Œë ¤ì¤˜",
        "ì»´ê³µ ì¶”ì²œ ê³¼ëª©ì€?",
        "ì›¹í”„ë¡œê·¸ë˜ë°ì´ë‘ ëª¨ë°”ì¼í”„ë¡œê·¸ë˜ë° ì¤‘ì— ë­ê°€ ë‚˜ì„ê¹Œ?"
    ]
    
    return jsonify({
        'message': 'ì±—ë´‡ APIê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!',
        'test_queries': test_messages,
        'endpoints': {
            'chat': 'POST /api/chat',
            'test': 'GET /api/chat/test'
        }
    })

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    provider_name = "Gemini" if LLM_PROVIDER == "gemini" else "OpenAI GPT-4"
    return f'''
    <h1>ğŸ¤– ì—ë¸Œë¦¬íƒ€ì„ AI ì±—ë´‡ API</h1>
    <p><strong>{provider_name}</strong>ë¥¼ ì´ìš©í•œ ê°•ì˜í‰ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸</p>
    <p>í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ LLM: <strong>{LLM_PROVIDER.upper()}</strong></p>
    
    <h2>ê¸°ëŠ¥:</h2>
    <ul>
        <li>ğŸ” ìì—°ì–´ë¡œ ê°•ì˜ ê²€ìƒ‰</li>
        <li>ğŸ“Š ê°•ì˜ ë¹„êµ ë° ë¶„ì„</li>
        <li>ğŸ’¡ ê°œì¸ ë§ì¶¤ ì¶”ì²œ</li>
        <li>ğŸ’¬ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤</li>
    </ul>
    
    <h2>ì‚¬ìš©ë²•:</h2>
    <ul>
        <li><code>POST /api/chat</code> - AI ì±—ë´‡ ëŒ€í™” (ê¸°ì¡´ Function Calling ë°©ì‹)</li>
        <li><code>POST /api/rag/chat</code> - RAG ê¸°ë°˜ AI ì±—ë´‡ ëŒ€í™” (Pinecone ë²¡í„° ê²€ìƒ‰)</li>
        <li><code>GET /api/chat/test</code> - í…ŒìŠ¤íŠ¸ í˜ì´ì§€</li>
        <li><code>GET /api/rag/chat/test</code> - RAG í…ŒìŠ¤íŠ¸ í˜ì´ì§€</li>
        <li><code>GET /api/rag/health</code> - RAG ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬</li>
    </ul>
    
    <h2>ì˜ˆì‹œ ì§ˆë¬¸:</h2>
    <ul>
        <li>"ë°ì´í„°ë² ì´ìŠ¤ ê°•ì˜í‰ ì•Œë ¤ì¤˜"</li>
        <li>"ì»´ê³µì—ì„œ ê¿€ê°• ì¶”ì²œí•´ì¤˜"</li>
        <li>"ê¹€êµìˆ˜ë‹˜ ê°•ì˜ ì–´ë–¤ì§€ ê¶ê¸ˆí•´"</li>
    </ul>
    
    <h2>ê¸°ì¡´ APIì™€ RAG APIì˜ ì°¨ì´:</h2>
    <ul>
        <li><strong>/api/chat</strong>: Function Callingìœ¼ë¡œ MongoDBì—ì„œ ì‹¤ì‹œê°„ ê²€ìƒ‰</li>
        <li><strong>/api/rag/chat</strong>: Pinecone ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬í•œ ê°•ì˜í‰ì„ ì°¾ì•„ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©</li>
    </ul>
    
    <h2>ì„¤ì • ë³€ê²½:</h2>
    <p>í™˜ê²½ë³€ìˆ˜ <code>LLM_PROVIDER</code>ë¥¼ ì„¤ì •í•˜ì—¬ LLMì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (gemini ë˜ëŠ” openai)</p>
    <p>RAG ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ <code>PINECONE_API_KEY</code>ì™€ <code>PINECONE_INDEX</code>ë¥¼ ì„¤ì •í•˜ì„¸ìš”</p>
    '''

@app.route('/api/health/db', methods=['GET'])
def health_db():
    """MongoDB ì—°ê²° í—¬ìŠ¤ì²´í¬"""
    try:
        db = get_mongo_db()
        result = db.command('ping')
        return jsonify({'ok': True, 'result': result}), 200
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)}), 500

# ========== RAG ì—”ë“œí¬ì¸íŠ¸ ==========

@app.route('/api/rag/chat', methods=['POST'])
def rag_chat():
    """RAG ê¸°ë°˜ AI ì±—ë´‡ ëŒ€í™” API (VectorStore ì—†ìœ¼ë©´ ì¼ë°˜ ì±—ë´‡ìœ¼ë¡œ í´ë°±)"""
    try:
        data = request.json
        user_message = data.get('message', '').strip()
        conversation_history = data.get('history', [])
        top_k = data.get('top_k', 5)  # ê²€ìƒ‰í•  ê°•ì˜í‰ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
        # Namespace: ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ None (Pineconeì´ ìë™ìœ¼ë¡œ _default_ ì‚¬ìš©)
        namespace = data.get('namespace') or os.getenv('PINE_NS') or None

        if not user_message:
            return jsonify({'error': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”'}), 400

        # VectorStoreê°€ ì—†ìœ¼ë©´ RAG ëŒ€ì‹  ì¼ë°˜ ì±—ë´‡ìœ¼ë¡œ í´ë°±
        if not vector_store:
            print("âš ï¸ VectorStore ë¯¸ì´ˆê¸°í™”: ì¼ë°˜ LLM ì±—ë´‡ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            if LLM_PROVIDER == 'openai':
                ai_response, _ = chat_with_openai(user_message, conversation_history)
            elif LLM_PROVIDER == 'gemini':
                ai_response, _ = chat_with_gemini(user_message, conversation_history)
            else:
                return jsonify({'error': f'ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM Provider: {LLM_PROVIDER}'}), 400

            return jsonify({
                'response': ai_response,
                'timestamp': datetime.now().isoformat(),
                'llm_provider': LLM_PROVIDER,
                'rag_enabled': False,
                'reviews_found': 0,
                'top_reviews': []
            })

        print(f"ğŸ’¬ [RAG] ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}")
        print(f"ğŸ¤– LLM Provider: {LLM_PROVIDER}")
        print(f"ğŸ” ê²€ìƒ‰í•  ê°•ì˜í‰ ê°œìˆ˜: {top_k}")
        # namespaceê°€ Noneì´ë©´ Pineconeì´ ìë™ìœ¼ë¡œ _default_ë¥¼ ì‚¬ìš©
        print(f"ğŸ“¦ Namespace: {namespace if namespace else '_default_ (ìë™)'}")

        # LLM Providerì— ë”°ë¼ ë‹¤ë¥¸ í•¨ìˆ˜ í˜¸ì¶œ
        if LLM_PROVIDER == 'openai':
            ai_response, similar_reviews = chat_with_rag_openai(user_message, conversation_history, top_k, namespace)
        elif LLM_PROVIDER == 'gemini':
            ai_response, similar_reviews = chat_with_rag_gemini(user_message, conversation_history, top_k, namespace)
        else:
            return jsonify({'error': f'ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM Provider: {LLM_PROVIDER}'}), 400

        print(f"ğŸ¤– [RAG] AI ì‘ë‹µ: {ai_response[:100]}...")
        print(f"ğŸ“Š ê²€ìƒ‰ëœ ê°•ì˜í‰ ê°œìˆ˜: {len(similar_reviews)}")

        # ê²€ìƒ‰ëœ ê°•ì˜í‰ì˜ ë©”íƒ€ë°ì´í„° ì •ë¦¬ (ë¯¼ê°í•œ ì •ë³´ ì œì™¸)
        review_summaries = []
        for review in similar_reviews[:3]:  # ìƒìœ„ 3ê°œë§Œ ë°˜í™˜
            metadata = review.get('metadata', {})
            review_summaries.append({
                'course_name': metadata.get('course_name', ''),
                'professor': metadata.get('professor', ''),
                'rating': metadata.get('rating', 0),
                'similarity_score': round(review.get('score', 0), 3)
            })

        return jsonify({
            'response': ai_response,
            'timestamp': datetime.now().isoformat(),
            'llm_provider': LLM_PROVIDER,
            'rag_enabled': True,
            'reviews_found': len(similar_reviews),
            'top_reviews': review_summaries
        })

    except Exception as e:
        print(f"âŒ RAG ì±—ë´‡ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'RAG ì±—ë´‡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}), 500

@app.route('/api/rag/chat/test', methods=['GET'])
def test_rag_chat():
    """RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    test_messages = [
        "ë°ì´í„°ë² ì´ìŠ¤ ê°•ì˜í‰ ì•Œë ¤ì¤˜",
        "ì»´ê³µ ì¶”ì²œ ê³¼ëª©ì€?",
        "ì›¹í”„ë¡œê·¸ë˜ë°ì´ë‘ ëª¨ë°”ì¼í”„ë¡œê·¸ë˜ë° ì¤‘ì— ë­ê°€ ë‚˜ì„ê¹Œ?",
        "íŒ€ í”„ë¡œì íŠ¸ê°€ ìˆëŠ” ê°•ì˜ ì¶”ì²œí•´ì¤˜"
    ]
    
    vector_store_status = "âœ… ì´ˆê¸°í™”ë¨" if vector_store else "âŒ ì´ˆê¸°í™” ì‹¤íŒ¨"
    
    return jsonify({
        'message': 'RAG ì±—ë´‡ APIê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!',
        'vector_store_status': vector_store_status,
        'llm_provider': LLM_PROVIDER,
        'test_queries': test_messages,
        'endpoints': {
            'rag_chat': 'POST /api/rag/chat',
            'test': 'GET /api/rag/chat/test',
            'health': 'GET /api/rag/health'
        }
    })

@app.route('/api/rag/health', methods=['GET'])
def rag_health():
    """RAG ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬"""
    try:
        health_status = {
            'vector_store': False,
            'llm_provider': LLM_PROVIDER,
            'pinecone_index': None,
            'index_stats': None
        }
        
        if vector_store:
            health_status['vector_store'] = True
            health_status['pinecone_index'] = vector_store.index_name
            try:
                stats = vector_store.get_index_stats()
                health_status['index_stats'] = stats
            except Exception as e:
                health_status['index_stats_error'] = str(e)
        
        return jsonify(health_status), 200
        
    except Exception as e:
        return jsonify({
            'error': str(e),
            'vector_store': False
        }), 500

if __name__ == '__main__':
    provider_name = "Gemini" if LLM_PROVIDER == "gemini" else "OpenAI GPT-4"
    vector_status = "âœ… ì—°ê²°ë¨" if vector_store else "âŒ ì—°ê²° ì‹¤íŒ¨"
    
    print("ğŸ¤– ì—ë¸Œë¦¬íƒ€ì„ AI ì±—ë´‡ API ì„œë²„ ì‹œì‘")
    print("ğŸ“ http://localhost:5003")
    print(f"ğŸ”§ LLM Provider: {LLM_PROVIDER.upper()}")
    print(f"ğŸ“Š Pinecone VectorStore: {vector_status}")
    if vector_store:
        print(f"   - ì¸ë±ìŠ¤: {vector_store.index_name}")
        print("   - RAG ì—”ë“œí¬ì¸íŠ¸: POST /api/rag/chat")
    
    app.run(debug=True, host='0.0.0.0', port=5003)
